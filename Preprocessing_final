#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Fri Dec 20 12:22:35 2019

#Preprocessing"""

# LOADING DATA
import pandas as pd
import nltk

data = pd.read_csv('isear.csv', sep = '|', encoding='latin-1')

# PREPROCESSING
#Subsetting data to relevant variables
data = data[['Field1', 'SIT', 'AGE', 'SEX', 'COUN']]

# Changing column names
data.rename(columns={'Field1': 'emotions', 'SIT': 'sentence'}, inplace=True)

# Check number of observation pr. emotion
data_obs = pd.crosstab(data['emotions'], columns='count')
print(data_obs)

# Labeling emotions to numbers - we first make a copy with original emotions
data['emotions_label'] =data['emotions']
data['emotions'] = data.emotions.map({'joy': 0, 'fear': 1, 'anger': 2, 'sadness': 3, 'disgust': 4, 'shame': 5, 'guilt': 6})

#Make a copy of original sentence before preprocessing of text variable
data['copy_sentence'] =data['sentence']

#Subset data so that guilt and shame are not included
#Subset data
data=data[(data['emotions']<5)]

# Check number of observations pr. emotion again
data_obs = pd.crosstab(data['emotions_label'], columns='count')
print(data_obs)

# %%
# PREPROCESSING
#Remove artefacts from data
data['sentence'] = data.sentence.str.replace('[ÌÁ]', '')
#Remove new line
data['sentence'] = data.sentence.str.replace('[\n]', '')
#Remove punctuation (not !, )
data['sentence'] = data.sentence.str.replace('[\.\[\]\,\'\"\?\:\;\-\_\=\(\)\|\*\@\#\&\$\"\/\%\+]', '')

# TOKENIZE AND LOWERCASE    
data['sentence'] = [
    [word for word in document.lower().split()]
    for document in data['sentence']]

# Lemmatizing by NLTK WordNet lemmatizer
from nltk.stem import WordNetLemmatizer 
lemmatizer = WordNetLemmatizer()
data['sentence'] = data['sentence'].apply(lambda x: [lemmatizer.lemmatize(y) for y in x])

# %%
import numpy as np

# Cleaning data before preprocessing - removing observations that are not meaningful for classification

#We first start by producing a tabel of the distributions of observations pr. emotion
data_freq = pd.crosstab(data['emotions_label'], columns='count')
print(data_freq)

#We manually go through data with 1-5 words to see if there are patterns of phrases to be excluded
#First we make a variable with word counts
data['word_count'] = data['sentence'].apply(lambda x: len(x))
  
#Deal with irrelevant answers
#Finding all instances of No response, never felt this, cannot think of anything etc. with regex
data['filter'] = np.where((data.copy_sentence.str.contains("not applicable|nothing.|no reponse.|do not remember any incident.|the same as in guilt.|never.|not included in questionnaire.|do not remember.|can not remember.|no description.|no response.|not applicaple.|no response|never felt this|can not think of anything|cannot think of anything|can't think of anything|can not recall|can not remember|can't recall|can't remember|as for disgust|same as in anger|can not think of any situation|never experienced|doesn't apply|blank", case = False)) & (data['word_count']<7),1,0)

#Eye-ball all true instances to see if we exclude anything we should not
#Delete observations that include relevant search words/phrases after eye-balling
data = data[data['filter'] != 1] 

#Append new count to table to see how many observations are deleted pr. emotion
data_freq['count_filtered'] = pd.crosstab(data['emotions_label'], columns='count')
data_freq['diff'] = data_freq['count'] - data_freq['count_filtered']
print(data_freq)

# %%

#Check whether we mention the emotion in the sentence
data['anger'] = np.where((data.copy_sentence.str.contains("anger|angry", case = False)) & (data['emotions'] <= 2),1,0)
data['disgust'] = np.where((data.copy_sentence.str.contains("disgust|disgusted", case = False)) & (data['emotions'] <= 4),1,0)
data['fear'] = np.where((data.copy_sentence.str.contains("fear|fearful", case = False)) & (data['emotions'] <= 1),1,0)
data['joy'] = np.where((data.copy_sentence.str.contains("joy|joyful", case = False)) & (data['emotions'] <= 0),1,0)
data['sadness'] = np.where((data.copy_sentence.str.contains("sadness|sad", case = False)) & (data['emotions'] <= 3),1,0)

#Count number of times for number of times the emotion occures in text
print(data['anger'].sum())
print(data['disgust'].sum())
print(data['fear'].sum())
print(data['joy'].sum())
print(data['sadness'].sum())


# %%
#Delete variables used for cleaning
data = data[['emotions','emotions_label','sentence','copy_sentence', 'word_count', 'AGE', 'SEX', 'COUN']]

#Make new variable with untokenized version of clean text data
data['sentence_ut']=data['sentence'].str.join(" ")


# %%
# SAVE CLEANED DATA
data.to_csv(r'emotiondata_cleaned.csv')


